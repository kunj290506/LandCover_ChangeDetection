{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ Land Cover Change Detection - Complete Training Pipeline\n",
                "\n",
                "## üìä Features:\n",
                "- ‚úÖ Real-time GPU monitoring\n",
                "- ‚úÖ Live training dashboard\n",
                "- ‚úÖ Go/No-Go checkpoints\n",
                "- ‚úÖ Prediction visualization\n",
                "- ‚úÖ Comprehensive metrics tracking"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Cell 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install -q tqdm matplotlib seaborn pandas scikit-learn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìö Cell 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import sys\n",
                "import time\n",
                "import warnings\n",
                "import subprocess\n",
                "from datetime import datetime, timedelta\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Tuple, Optional\n",
                "from dataclasses import dataclass\n",
                "from collections import defaultdict\n",
                "import random\n",
                "\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from PIL import Image\n",
                "from tqdm.auto import tqdm\n",
                "from IPython.display import display, clear_output, HTML\n",
                "import pandas as pd\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.nn.functional as F\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torch import optim\n",
                "import torchvision.transforms.functional as TF\n",
                "\n",
                "from sklearn.metrics import f1_score, jaccard_score, cohen_kappa_score, precision_score, recall_score\n",
                "\n",
                "warnings.filterwarnings('ignore')\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚öôÔ∏è Cell 3: Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfig:\n",
                "    \"\"\"All training hyperparameters\"\"\"\n",
                "    # Data paths - YOUR KAGGLE PATH\n",
                "    data_root: str = \"/kaggle/input/dataset1new\"\n",
                "    train_list: str = \"/kaggle/working/train_list.txt\"\n",
                "    val_list: str = \"/kaggle/working/val_list.txt\"\n",
                "    test_list: str = \"/kaggle/working/test_list.txt\"\n",
                "    patch_size: int = 256\n",
                "    \n",
                "    # Training\n",
                "    batch_size: int = 16\n",
                "    num_workers: int = 4\n",
                "    pin_memory: bool = True\n",
                "    epochs: int = 100\n",
                "    learning_rate: float = 3e-4\n",
                "    weight_decay: float = 1e-4\n",
                "    \n",
                "    # Model\n",
                "    base_channel: int = 32\n",
                "    use_attention: bool = True\n",
                "    \n",
                "    # Loss\n",
                "    bce_weight: float = 0.7\n",
                "    dice_weight: float = 0.3\n",
                "    focal_gamma: float = 2.0\n",
                "    use_focal: bool = True\n",
                "    \n",
                "    # Early Stopping & Checkpoints\n",
                "    patience: int = 15\n",
                "    checkpoint_dir: str = \"/kaggle/working/checkpoints\"\n",
                "    save_every: int = 5\n",
                "    log_interval: int = 10\n",
                "    visualize_interval: int = 5\n",
                "    \n",
                "    # Go/No-Go Thresholds\n",
                "    epoch_3_f1_threshold: float = 0.60\n",
                "    epoch_10_f1_threshold: float = 0.70\n",
                "    overfitting_patience: int = 5\n",
                "    \n",
                "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "\n",
                "config = TrainingConfig()\n",
                "os.makedirs(config.checkpoint_dir, exist_ok=True)\n",
                "print(\"‚úÖ Configuration loaded\")\n",
                "print(f\"üì± Device: {config.device}\")\n",
                "print(f\"üìÇ Data Root: {config.data_root}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîß Cell 4: Utility Classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AverageMeter:\n",
                "    \"\"\"Computes and stores average values\"\"\"\n",
                "    def __init__(self, name: str = \"\"):\n",
                "        self.name = name\n",
                "        self.reset()\n",
                "\n",
                "    def reset(self):\n",
                "        self.val = self.avg = self.sum = self.count = 0\n",
                "        self.history = []\n",
                "\n",
                "    def update(self, val, n=1):\n",
                "        self.val = val\n",
                "        self.sum += val * n\n",
                "        self.count += n\n",
                "        self.avg = self.sum / self.count\n",
                "        self.history.append(val)\n",
                "\n",
                "\n",
                "class GPUMonitor:\n",
                "    \"\"\"Real-time GPU monitoring\"\"\"\n",
                "    def __init__(self):\n",
                "        self.utilization_history = []\n",
                "        self.memory_history = []\n",
                "        \n",
                "    def get_gpu_stats(self) -> Dict:\n",
                "        try:\n",
                "            result = subprocess.run(\n",
                "                ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total,temperature.gpu',\n",
                "                 '--format=csv,noheader,nounits'],\n",
                "                capture_output=True, text=True, timeout=5\n",
                "            )\n",
                "            if result.returncode == 0:\n",
                "                values = result.stdout.strip().split(', ')\n",
                "                stats = {\n",
                "                    'gpu_util': float(values[0]),\n",
                "                    'memory_used': float(values[1]),\n",
                "                    'memory_total': float(values[2]),\n",
                "                    'temperature': float(values[3])\n",
                "                }\n",
                "                self.utilization_history.append(stats['gpu_util'])\n",
                "                return stats\n",
                "        except:\n",
                "            pass\n",
                "        return {'gpu_util': 0, 'memory_used': 0, 'memory_total': 0, 'temperature': 0}\n",
                "    \n",
                "    def get_avg_utilization(self) -> float:\n",
                "        return np.mean(self.utilization_history) if self.utilization_history else 0\n",
                "\n",
                "\n",
                "class TrainingLogger:\n",
                "    \"\"\"Comprehensive training logger\"\"\"\n",
                "    def __init__(self, config):\n",
                "        self.config = config\n",
                "        self.metrics_history = defaultdict(list)\n",
                "        self.epoch_times = []\n",
                "        self.gpu_monitor = GPUMonitor()\n",
                "        \n",
                "    def log_epoch(self, epoch, train_metrics, val_metrics, epoch_time, lr):\n",
                "        self.epoch_times.append(epoch_time)\n",
                "        self.metrics_history['epoch'].append(epoch)\n",
                "        self.metrics_history['train_loss'].append(train_metrics['loss'])\n",
                "        self.metrics_history['val_loss'].append(val_metrics['loss'])\n",
                "        self.metrics_history['val_f1'].append(val_metrics['f1'])\n",
                "        self.metrics_history['val_iou'].append(val_metrics['iou'])\n",
                "        self.metrics_history['val_precision'].append(val_metrics['precision'])\n",
                "        self.metrics_history['val_recall'].append(val_metrics['recall'])\n",
                "        self.metrics_history['lr'].append(lr)\n",
                "        self.metrics_history['epoch_time'].append(epoch_time)\n",
                "        self.metrics_history['loss_gap'].append(abs(train_metrics['loss'] - val_metrics['loss']))\n",
                "        \n",
                "    def get_dataframe(self):\n",
                "        return pd.DataFrame(self.metrics_history)\n",
                "    \n",
                "    def estimate_remaining_time(self, current_epoch, total_epochs):\n",
                "        if not self.epoch_times:\n",
                "            return \"Calculating...\"\n",
                "        avg_time = np.mean(self.epoch_times)\n",
                "        remaining = (total_epochs - current_epoch) * avg_time\n",
                "        return str(timedelta(seconds=int(remaining)))\n",
                "\n",
                "print(\"‚úÖ Utility classes loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üì¶ Cell 5: Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ChangeDetectionDataset(Dataset):\n",
                "    \"\"\"Optimized dataset for change detection\"\"\"\n",
                "    def __init__(self, root_dir, list_path, mode='train', patch_size=256):\n",
                "        self.root_dir = root_dir\n",
                "        self.mode = mode\n",
                "        self.patch_size = patch_size\n",
                "        \n",
                "        self.files = []\n",
                "        with open(list_path, 'r') as f:\n",
                "            for line in f:\n",
                "                parts = line.strip().split()\n",
                "                if len(parts) >= 3:\n",
                "                    self.files.append(parts)\n",
                "        \n",
                "        self.mean = [0.485, 0.456, 0.406]\n",
                "        self.std = [0.229, 0.224, 0.225]\n",
                "        print(f\"üìÇ Loaded {len(self.files)} samples for {mode}\")\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.files)\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        img1_path = os.path.join(self.root_dir, self.files[idx][0])\n",
                "        img2_path = os.path.join(self.root_dir, self.files[idx][1])\n",
                "        label_path = os.path.join(self.root_dir, self.files[idx][2])\n",
                "\n",
                "        img1 = Image.open(img1_path).convert('RGB')\n",
                "        img2 = Image.open(img2_path).convert('RGB')\n",
                "        label = Image.open(label_path).convert('L')\n",
                "\n",
                "        if self.mode == 'train':\n",
                "            img1, img2, label = self._augment(img1, img2, label)\n",
                "        \n",
                "        img1 = TF.normalize(TF.to_tensor(img1), self.mean, self.std)\n",
                "        img2 = TF.normalize(TF.to_tensor(img2), self.mean, self.std)\n",
                "        label = TF.to_tensor(label)\n",
                "\n",
                "        return {'image1': img1, 'image2': img2, 'label': label, 'name': self.files[idx][0]}\n",
                "\n",
                "    def _augment(self, img1, img2, label):\n",
                "        if random.random() > 0.5:\n",
                "            img1, img2, label = TF.hflip(img1), TF.hflip(img2), TF.hflip(label)\n",
                "        if random.random() > 0.5:\n",
                "            img1, img2, label = TF.vflip(img1), TF.vflip(img2), TF.vflip(label)\n",
                "        if random.random() > 0.5:\n",
                "            angle = random.choice([90, 180, 270])\n",
                "            img1, img2, label = TF.rotate(img1, angle), TF.rotate(img2, angle), TF.rotate(label, angle)\n",
                "        return img1, img2, label\n",
                "\n",
                "print(\"‚úÖ Dataset class loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üß† Cell 6: SNUNet Model with CBAM"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ChannelAttention(nn.Module):\n",
                "    def __init__(self, in_planes, ratio=16):\n",
                "        super().__init__()\n",
                "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
                "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
                "        self.fc = nn.Sequential(\n",
                "            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n",
                "            nn.ReLU(),\n",
                "            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
                "        )\n",
                "        self.sigmoid = nn.Sigmoid()\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.sigmoid(self.fc(self.avg_pool(x)) + self.fc(self.max_pool(x)))\n",
                "\n",
                "\n",
                "class SpatialAttention(nn.Module):\n",
                "    def __init__(self, kernel_size=7):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
                "        self.sigmoid = nn.Sigmoid()\n",
                "\n",
                "    def forward(self, x):\n",
                "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
                "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
                "        return self.sigmoid(self.conv(torch.cat([avg_out, max_out], dim=1)))\n",
                "\n",
                "\n",
                "class CBAM(nn.Module):\n",
                "    def __init__(self, in_planes):\n",
                "        super().__init__()\n",
                "        self.ca = ChannelAttention(in_planes)\n",
                "        self.sa = SpatialAttention()\n",
                "\n",
                "    def forward(self, x):\n",
                "        return x * self.sa(x * self.ca(x))\n",
                "\n",
                "\n",
                "class ConvBlock(nn.Module):\n",
                "    def __init__(self, in_ch, out_ch, use_cbam=False):\n",
                "        super().__init__()\n",
                "        self.conv = nn.Sequential(\n",
                "            nn.Conv2d(in_ch, out_ch, 3, padding=1, bias=False),\n",
                "            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True),\n",
                "            nn.Conv2d(out_ch, out_ch, 3, padding=1, bias=False),\n",
                "            nn.BatchNorm2d(out_ch), nn.ReLU(inplace=True)\n",
                "        )\n",
                "        self.cbam = CBAM(out_ch) if use_cbam else None\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.conv(x)\n",
                "        return self.cbam(x) if self.cbam else x\n",
                "\n",
                "\n",
                "class SNUNet(nn.Module):\n",
                "    def __init__(self, in_ch=3, num_classes=1, C=32, use_attn=True):\n",
                "        super().__init__()\n",
                "        # Encoder\n",
                "        self.conv0_0 = ConvBlock(in_ch, C)\n",
                "        self.pool1 = nn.MaxPool2d(2)\n",
                "        self.conv1_0 = ConvBlock(C, C*2)\n",
                "        self.pool2 = nn.MaxPool2d(2)\n",
                "        self.conv2_0 = ConvBlock(C*2, C*4)\n",
                "        self.pool3 = nn.MaxPool2d(2)\n",
                "        self.conv3_0 = ConvBlock(C*4, C*8)\n",
                "        self.pool4 = nn.MaxPool2d(2)\n",
                "        self.conv4_0 = ConvBlock(C*8, C*16)\n",
                "        \n",
                "        # Decoder\n",
                "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
                "        self.conv0_1 = ConvBlock(C*2 + C*4, C, use_cbam=use_attn)\n",
                "        self.conv1_1 = ConvBlock(C*4 + C*8, C*2, use_cbam=use_attn)\n",
                "        self.conv2_1 = ConvBlock(C*8 + C*16, C*4, use_cbam=use_attn)\n",
                "        self.conv3_1 = ConvBlock(C*16 + C*32, C*8, use_cbam=use_attn)\n",
                "        self.conv0_2 = ConvBlock(C*2 + C*2 + C, C, use_cbam=use_attn)\n",
                "        self.conv1_2 = ConvBlock(C*4 + C*4 + C*2, C*2, use_cbam=use_attn)\n",
                "        self.conv2_2 = ConvBlock(C*8 + C*8 + C*4, C*4, use_cbam=use_attn)\n",
                "        self.conv0_3 = ConvBlock(C*2 + C*2 + C + C, C, use_cbam=use_attn)\n",
                "        self.conv1_3 = ConvBlock(C*4 + C*4 + C*2 + C*2, C*2, use_cbam=use_attn)\n",
                "        self.conv0_4 = ConvBlock(C*2 + C*2 + C + C + C, C)\n",
                "        self.final = nn.Conv2d(C, num_classes, 1)\n",
                "        \n",
                "    def forward(self, x1, x2):\n",
                "        # Encoder 1\n",
                "        x1_0_0 = self.conv0_0(x1)\n",
                "        x1_1_0 = self.conv1_0(self.pool1(x1_0_0))\n",
                "        x1_2_0 = self.conv2_0(self.pool2(x1_1_0))\n",
                "        x1_3_0 = self.conv3_0(self.pool3(x1_2_0))\n",
                "        x1_4_0 = self.conv4_0(self.pool4(x1_3_0))\n",
                "        # Encoder 2\n",
                "        x2_0_0 = self.conv0_0(x2)\n",
                "        x2_1_0 = self.conv1_0(self.pool1(x2_0_0))\n",
                "        x2_2_0 = self.conv2_0(self.pool2(x2_1_0))\n",
                "        x2_3_0 = self.conv3_0(self.pool3(x2_2_0))\n",
                "        x2_4_0 = self.conv4_0(self.pool4(x2_3_0))\n",
                "        # Decoder\n",
                "        x0_1 = self.conv0_1(torch.cat([x1_0_0, x2_0_0, self.up(x1_1_0), self.up(x2_1_0)], 1))\n",
                "        x1_1 = self.conv1_1(torch.cat([x1_1_0, x2_1_0, self.up(x1_2_0), self.up(x2_2_0)], 1))\n",
                "        x2_1 = self.conv2_1(torch.cat([x1_2_0, x2_2_0, self.up(x1_3_0), self.up(x2_3_0)], 1))\n",
                "        x3_1 = self.conv3_1(torch.cat([x1_3_0, x2_3_0, self.up(x1_4_0), self.up(x2_4_0)], 1))\n",
                "        x0_2 = self.conv0_2(torch.cat([x1_0_0, x2_0_0, x0_1, self.up(x1_1)], 1))\n",
                "        x1_2 = self.conv1_2(torch.cat([x1_1_0, x2_1_0, x1_1, self.up(x2_1)], 1))\n",
                "        x2_2 = self.conv2_2(torch.cat([x1_2_0, x2_2_0, x2_1, self.up(x3_1)], 1))\n",
                "        x0_3 = self.conv0_3(torch.cat([x1_0_0, x2_0_0, x0_1, x0_2, self.up(x1_2)], 1))\n",
                "        x1_3 = self.conv1_3(torch.cat([x1_1_0, x2_1_0, x1_1, x1_2, self.up(x2_2)], 1))\n",
                "        x0_4 = self.conv0_4(torch.cat([x1_0_0, x2_0_0, x0_1, x0_2, x0_3, self.up(x1_3)], 1))\n",
                "        return self.final(x0_4)\n",
                "\n",
                "print(\"‚úÖ SNUNet model loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìâ Cell 7: Loss Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DiceLoss(nn.Module):\n",
                "    def __init__(self, smooth=1.0):\n",
                "        super().__init__()\n",
                "        self.smooth = smooth\n",
                "\n",
                "    def forward(self, inputs, targets):\n",
                "        inputs = torch.sigmoid(inputs).view(-1)\n",
                "        targets = targets.view(-1)\n",
                "        intersection = (inputs * targets).sum()\n",
                "        return 1 - (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
                "\n",
                "\n",
                "class FocalLoss(nn.Module):\n",
                "    def __init__(self, alpha=0.25, gamma=2.0):\n",
                "        super().__init__()\n",
                "        self.alpha = alpha\n",
                "        self.gamma = gamma\n",
                "\n",
                "    def forward(self, inputs, targets):\n",
                "        bce = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
                "        pt = torch.exp(-bce)\n",
                "        return (self.alpha * (1 - pt) ** self.gamma * bce).mean()\n",
                "\n",
                "\n",
                "class HybridLoss(nn.Module):\n",
                "    def __init__(self, bce_weight=0.7, dice_weight=0.3, use_focal=True, gamma=2.0):\n",
                "        super().__init__()\n",
                "        self.bce_weight = bce_weight\n",
                "        self.dice_weight = dice_weight\n",
                "        self.dice = DiceLoss()\n",
                "        self.focal = FocalLoss(gamma=gamma) if use_focal else None\n",
                "        self.use_focal = use_focal\n",
                "\n",
                "    def forward(self, inputs, targets):\n",
                "        ce = self.focal(inputs, targets) if self.use_focal else F.binary_cross_entropy_with_logits(inputs, targets)\n",
                "        return self.bce_weight * ce + self.dice_weight * self.dice(inputs, targets)\n",
                "\n",
                "print(\"‚úÖ Loss functions loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìà Cell 8: Visualization Dashboard"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def display_dashboard(logger, epoch, total_epochs, gpu_stats, checkpoint_dir):\n",
                "    \"\"\"Display real-time training dashboard\"\"\"\n",
                "    clear_output(wait=True)\n",
                "    df = logger.get_dataframe()\n",
                "    if len(df) == 0:\n",
                "        return\n",
                "    \n",
                "    plt.style.use('dark_background')\n",
                "    fig = plt.figure(figsize=(16, 10))\n",
                "    fig.suptitle(f'üöÄ Training Dashboard - Epoch {epoch}/{total_epochs}', fontsize=16, fontweight='bold')\n",
                "    gs = fig.add_gridspec(3, 4, hspace=0.35, wspace=0.3)\n",
                "    \n",
                "    # Loss Curves\n",
                "    ax1 = fig.add_subplot(gs[0, :2])\n",
                "    ax1.plot(df['epoch'], df['train_loss'], 'b-', label='Train', lw=2)\n",
                "    ax1.plot(df['epoch'], df['val_loss'], 'r-', label='Val', lw=2)\n",
                "    ax1.fill_between(df['epoch'], df['train_loss'], df['val_loss'], alpha=0.2, color='yellow')\n",
                "    ax1.set_xlabel('Epoch'); ax1.set_ylabel('Loss'); ax1.set_title('üìâ Loss Curves')\n",
                "    ax1.legend(); ax1.grid(True, alpha=0.3)\n",
                "    \n",
                "    # F1 & IoU\n",
                "    ax2 = fig.add_subplot(gs[0, 2:])\n",
                "    ax2.plot(df['epoch'], df['val_f1'], 'g-', label='F1', lw=2, marker='o', ms=4)\n",
                "    ax2.plot(df['epoch'], df['val_iou'], 'm-', label='IoU', lw=2, marker='s', ms=4)\n",
                "    ax2.axhline(y=0.60, color='yellow', ls='--', alpha=0.5, label='Epoch3 Target')\n",
                "    ax2.axhline(y=0.70, color='orange', ls='--', alpha=0.5, label='Epoch10 Target')\n",
                "    ax2.axhline(y=0.85, color='lime', ls='--', alpha=0.5, label='Final Target')\n",
                "    ax2.set_xlabel('Epoch'); ax2.set_ylabel('Score'); ax2.set_title('üìä F1 & IoU')\n",
                "    ax2.legend(loc='lower right'); ax2.grid(True, alpha=0.3); ax2.set_ylim(0, 1)\n",
                "    \n",
                "    # Precision & Recall\n",
                "    ax3 = fig.add_subplot(gs[1, :2])\n",
                "    ax3.plot(df['epoch'], df['val_precision'], 'c-', label='Precision', lw=2)\n",
                "    ax3.plot(df['epoch'], df['val_recall'], 'y-', label='Recall', lw=2)\n",
                "    ax3.set_xlabel('Epoch'); ax3.set_ylabel('Score'); ax3.set_title('üéØ Precision & Recall')\n",
                "    ax3.legend(); ax3.grid(True, alpha=0.3); ax3.set_ylim(0, 1)\n",
                "    \n",
                "    # Learning Rate\n",
                "    ax4 = fig.add_subplot(gs[1, 2])\n",
                "    ax4.plot(df['epoch'], df['lr'], 'orange', lw=2)\n",
                "    ax4.set_xlabel('Epoch'); ax4.set_ylabel('LR'); ax4.set_title('üìà Learning Rate')\n",
                "    ax4.grid(True, alpha=0.3)\n",
                "    \n",
                "    # Loss Gap\n",
                "    ax5 = fig.add_subplot(gs[1, 3])\n",
                "    colors = ['green' if g < 0.3 else 'red' for g in df['loss_gap']]\n",
                "    ax5.bar(df['epoch'], df['loss_gap'], color=colors, alpha=0.7)\n",
                "    ax5.axhline(y=0.3, color='red', ls='--', label='Overfit Threshold')\n",
                "    ax5.set_xlabel('Epoch'); ax5.set_ylabel('Gap'); ax5.set_title('‚ö†Ô∏è Loss Gap')\n",
                "    ax5.grid(True, alpha=0.3)\n",
                "    \n",
                "    # GPU Stats\n",
                "    ax6 = fig.add_subplot(gs[2, 0])\n",
                "    ax6.axis('off')\n",
                "    gpu_text = f\"üñ•Ô∏è GPU MONITOR\\n{'‚îÅ'*16}\\nUtil: {gpu_stats['gpu_util']:.1f}%\\nMem: {gpu_stats['memory_used']:.0f}/{gpu_stats['memory_total']:.0f}MB\\nTemp: {gpu_stats['temperature']:.0f}¬∞C\"\n",
                "    ax6.text(0.1, 0.5, gpu_text, fontsize=11, family='monospace', va='center', color='cyan')\n",
                "    \n",
                "    # Status\n",
                "    ax7 = fig.add_subplot(gs[2, 1])\n",
                "    ax7.axis('off')\n",
                "    best_f1 = df['val_f1'].max()\n",
                "    status_color = 'red' if (epoch >= 3 and best_f1 < 0.60) else ('orange' if (epoch >= 10 and best_f1 < 0.70) else 'lime')\n",
                "    remaining = logger.estimate_remaining_time(epoch, total_epochs)\n",
                "    status_text = f\"üìä STATUS\\n{'‚îÅ'*16}\\nEpoch: {epoch}/{total_epochs}\\nBest F1: {best_f1:.4f}\\nBest IoU: {df['val_iou'].max():.4f}\\nETA: {remaining}\"\n",
                "    ax7.text(0.1, 0.5, status_text, fontsize=11, family='monospace', va='center', color=status_color)\n",
                "    \n",
                "    # Checkpoints\n",
                "    ax8 = fig.add_subplot(gs[2, 2:])\n",
                "    ax8.axis('off')\n",
                "    checks = []\n",
                "    if epoch >= 3:\n",
                "        f1_3 = df[df['epoch'] <= 3]['val_f1'].max()\n",
                "        checks.append(f\"Epoch 3 (F1>0.60): {'‚úÖ' if f1_3 >= 0.60 else '‚ùå'} ({f1_3:.3f})\")\n",
                "    if epoch >= 10:\n",
                "        f1_10 = df[df['epoch'] <= 10]['val_f1'].max()\n",
                "        checks.append(f\"Epoch 10 (F1>0.70): {'‚úÖ' if f1_10 >= 0.70 else '‚ùå'} ({f1_10:.3f})\")\n",
                "    check_text = \"üìã GO/NO-GO CHECKPOINTS\\n\" + \"‚îÅ\"*24 + \"\\n\" + \"\\n\".join(checks) if checks else \"üìã Checkpoints pending...\"\n",
                "    ax8.text(0.1, 0.5, check_text, fontsize=10, family='monospace', va='center', color='white')\n",
                "    \n",
                "    plt.tight_layout()\n",
                "    plt.savefig(os.path.join(checkpoint_dir, 'dashboard.png'), dpi=100, facecolor='black')\n",
                "    plt.show()\n",
                "\n",
                "print(\"‚úÖ Visualization loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üèãÔ∏è Cell 9: Training & Validation Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_one_epoch(model, loader, criterion, optimizer, device, logger, log_interval=10):\n",
                "    model.train()\n",
                "    losses = AverageMeter()\n",
                "    pbar = tqdm(loader, desc='Training', leave=False)\n",
                "    \n",
                "    for i, batch in enumerate(pbar):\n",
                "        img1 = batch['image1'].to(device, non_blocking=True)\n",
                "        img2 = batch['image2'].to(device, non_blocking=True)\n",
                "        label = batch['label'].to(device, non_blocking=True)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        output = model(img1, img2)\n",
                "        loss = criterion(output, label)\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
                "        optimizer.step()\n",
                "        \n",
                "        losses.update(loss.item(), img1.size(0))\n",
                "        if i % log_interval == 0:\n",
                "            gpu = logger.gpu_monitor.get_gpu_stats()\n",
                "            pbar.set_postfix({'Loss': f'{losses.avg:.4f}', 'GPU': f'{gpu[\"gpu_util\"]:.0f}%'})\n",
                "    \n",
                "    return {'loss': losses.avg}\n",
                "\n",
                "\n",
                "def validate(model, loader, criterion, device):\n",
                "    model.eval()\n",
                "    losses = AverageMeter()\n",
                "    all_preds, all_targets = [], []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(loader, desc='Validating', leave=False):\n",
                "            img1 = batch['image1'].to(device, non_blocking=True)\n",
                "            img2 = batch['image2'].to(device, non_blocking=True)\n",
                "            label = batch['label'].to(device, non_blocking=True)\n",
                "            \n",
                "            output = model(img1, img2)\n",
                "            losses.update(criterion(output, label).item(), img1.size(0))\n",
                "            all_preds.append((torch.sigmoid(output) > 0.5).cpu())\n",
                "            all_targets.append(label.cpu())\n",
                "    \n",
                "    preds = torch.cat(all_preds).numpy().flatten().astype(int)\n",
                "    targets = torch.cat(all_targets).numpy().flatten().astype(int)\n",
                "    \n",
                "    return {\n",
                "        'loss': losses.avg,\n",
                "        'f1': f1_score(targets, preds, zero_division=0),\n",
                "        'iou': jaccard_score(targets, preds, zero_division=0),\n",
                "        'precision': precision_score(targets, preds, zero_division=0),\n",
                "        'recall': recall_score(targets, preds, zero_division=0),\n",
                "        'kappa': cohen_kappa_score(targets, preds) if len(np.unique(preds)) > 1 else 0\n",
                "    }\n",
                "\n",
                "\n",
                "def check_go_nogo(epoch, val_f1, config, val_loss_history):\n",
                "    if epoch == 3 and val_f1 < config.epoch_3_f1_threshold:\n",
                "        return False, f\"üö® STOP: Epoch 3 F1 ({val_f1:.3f}) < {config.epoch_3_f1_threshold}\"\n",
                "    if epoch in [10, 15] and val_f1 < config.epoch_10_f1_threshold:\n",
                "        return True, f\"‚ö†Ô∏è WARNING: Epoch {epoch} F1 ({val_f1:.3f}) < {config.epoch_10_f1_threshold}\"\n",
                "    if len(val_loss_history) >= config.overfitting_patience:\n",
                "        recent = val_loss_history[-config.overfitting_patience:]\n",
                "        if all(recent[i] > recent[i-1] for i in range(1, len(recent))):\n",
                "            return False, f\"üõë STOP: Overfitting detected!\"\n",
                "    return True, \"‚úÖ Training OK\"\n",
                "\n",
                "print(\"‚úÖ Training functions loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üöÄ Cell 10: Main Training Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train(config):\n",
                "    print(\"=\"*70)\n",
                "    print(\"üöÄ LAND COVER CHANGE DETECTION TRAINING\")\n",
                "    print(\"=\"*70)\n",
                "    \n",
                "    device = torch.device(config.device)\n",
                "    \n",
                "    # Data\n",
                "    train_ds = ChangeDetectionDataset(config.data_root, config.train_list, 'train', config.patch_size)\n",
                "    val_ds = ChangeDetectionDataset(config.data_root, config.val_list, 'val', config.patch_size)\n",
                "    \n",
                "    train_loader = DataLoader(train_ds, batch_size=config.batch_size, shuffle=True,\n",
                "                              num_workers=config.num_workers, pin_memory=config.pin_memory, drop_last=True)\n",
                "    val_loader = DataLoader(val_ds, batch_size=config.batch_size, shuffle=False,\n",
                "                            num_workers=config.num_workers, pin_memory=config.pin_memory)\n",
                "    \n",
                "    print(f\"üìä Train: {len(train_ds)} samples | Val: {len(val_ds)} samples\")\n",
                "    \n",
                "    # Model\n",
                "    model = SNUNet(3, 1, config.base_channel, config.use_attention).to(device)\n",
                "    print(f\"üìä Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
                "    \n",
                "    criterion = HybridLoss(config.bce_weight, config.dice_weight, config.use_focal, config.focal_gamma)\n",
                "    optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
                "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=1e-6)\n",
                "    \n",
                "    logger = TrainingLogger(config)\n",
                "    best_f1 = 0\n",
                "    val_loss_history = []\n",
                "    \n",
                "    # Sanity Check\n",
                "    print(\"\\nüî¨ PIPELINE SANITY CHECK\")\n",
                "    batch = next(iter(train_loader))\n",
                "    with torch.no_grad():\n",
                "        loss = criterion(model(batch['image1'].to(device), batch['image2'].to(device)), batch['label'].to(device))\n",
                "    gpu = logger.gpu_monitor.get_gpu_stats()\n",
                "    print(f\"üìä First batch loss: {loss.item():.4f} | GPU: {gpu['gpu_util']:.1f}%\")\n",
                "    \n",
                "    # Training Loop\n",
                "    print(\"\\nüèãÔ∏è STARTING TRAINING\")\n",
                "    for epoch in range(1, config.epochs + 1):\n",
                "        epoch_start = time.time()\n",
                "        lr = optimizer.param_groups[0]['lr']\n",
                "        \n",
                "        train_metrics = train_one_epoch(model, train_loader, criterion, optimizer, device, logger, config.log_interval)\n",
                "        val_metrics = validate(model, val_loader, criterion, device)\n",
                "        scheduler.step()\n",
                "        \n",
                "        epoch_time = time.time() - epoch_start\n",
                "        logger.log_epoch(epoch, train_metrics, val_metrics, epoch_time, lr)\n",
                "        val_loss_history.append(val_metrics['loss'])\n",
                "        \n",
                "        gpu = logger.gpu_monitor.get_gpu_stats()\n",
                "        display_dashboard(logger, epoch, config.epochs, gpu, config.checkpoint_dir)\n",
                "        \n",
                "        # Go/No-Go Check\n",
                "        should_continue, msg = check_go_nogo(epoch, val_metrics['f1'], config, val_loss_history)\n",
                "        print(msg)\n",
                "        if not should_continue:\n",
                "            print(\"üõë Training stopped!\")\n",
                "            break\n",
                "        \n",
                "        # Save best\n",
                "        if val_metrics['f1'] > best_f1:\n",
                "            best_f1 = val_metrics['f1']\n",
                "            torch.save({'epoch': epoch, 'model': model.state_dict(), 'best_f1': best_f1},\n",
                "                      os.path.join(config.checkpoint_dir, 'best_model.pth'))\n",
                "            print(f\"üíæ New best! F1: {best_f1:.4f}\")\n",
                "        \n",
                "        print(f\"üìä E{epoch} | Loss: {train_metrics['loss']:.4f}/{val_metrics['loss']:.4f} | F1: {val_metrics['f1']:.4f} | IoU: {val_metrics['iou']:.4f} | {epoch_time:.1f}s\")\n",
                "    \n",
                "    logger.get_dataframe().to_csv(os.path.join(config.checkpoint_dir, 'history.csv'), index=False)\n",
                "    print(f\"\\nüèÜ Training Complete! Best F1: {best_f1:.4f}\")\n",
                "    return model, logger\n",
                "\n",
                "print(\"‚úÖ Main training function loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ‚ñ∂Ô∏è Cell 11: RUN TRAINING"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# üöÄ START TRAINING\n",
                "model, logger = train(config)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üìä Cell 12: Final Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load and display training history\n",
                "df = pd.read_csv(os.path.join(config.checkpoint_dir, 'history.csv'))\n",
                "print(\"üìä Training Summary:\")\n",
                "print(df.describe())\n",
                "\n",
                "# Best metrics\n",
                "best_idx = df['val_f1'].idxmax()\n",
                "print(f\"\\nüèÜ Best Epoch: {df.loc[best_idx, 'epoch']}\")\n",
                "print(f\"   F1: {df.loc[best_idx, 'val_f1']:.4f}\")\n",
                "print(f\"   IoU: {df.loc[best_idx, 'val_iou']:.4f}\")\n",
                "print(f\"   Precision: {df.loc[best_idx, 'val_precision']:.4f}\")\n",
                "print(f\"   Recall: {df.loc[best_idx, 'val_recall']:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}